---
title: "Echantillonnage préférentiel"
author: "Pierre Gloaguen"
date: "02/04/2020"
output:
  beamer_presentation:
    dev: cairo_pdf
    includes:
      in_header: diapos_headers.tex
  slidy_presentation: default
---


```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE,
                      cache = TRUE,
                      fig.width =  7, 
                      fig.height = 3)
library(tidyverse)
```


```{r theme_set, cache = FALSE}
theme_set(theme_bw() +
            theme(
              panel.border = element_rect(colour = "black",
                                          fill = rgb(0, 0, 0, 0)),
              panel.grid = element_line(linetype = 2),
              plot.background = element_rect(fill = "white"),# bg around panel
              legend.background = element_blank(),
              text = element_text(family = "LM Roman 10", size = 12, face = "bold"),
              axis.title = element_text(size = rel(1.1)),
              legend.text = element_text(size = rel(1)),
              legend.title = element_text(size = rel(1.1)),
              plot.subtitle = element_text(hjust = 0.5, size = rel(1)),
              strip.background = element_rect(fill = "lightgoldenrod1"),
              plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5)))
```

## Annonces:

- Premier rendu pour le 13 Avril (exo5 du TD1)
- À faire en binome (à compléter sur le lien envoyé par mail)
- Corrections des exos 1 à 4 du TD1 mis en ligne
- On regardera ces corrections et vos questions aujourd'hui

## Rappel de l'épisode précédent

- Principes des méthodes de Monte Carlo;
- Approximation d'intégrales (espérances) par simulation;
- Fonctionne grâce à la loi des grands nombres, IC grâce au TCL.

<!-- ```{r figure_rappel, out.width="100%"} -->
<!-- knitr::include_graphics("diapos_monte_carlo_files/figure-beamer/plot_IC-1.pdf") -->
<!-- ``` -->

## Objectif du cours 

- Présentation de l'échantillonnage préférentiel
    - Extension naturelle des méthodes de MC simples;
    - Améliore l'efficacité dans certains cas;
    - Utile quand on ne sait pas simuler selon une loi donnée;\pause
- Motivation: cas de Monte Carlo problématiques;
- Définition;
- Propriétés: Analyse de la variance de ce nouvel estimateur;
- Illustration.

# Echantillonnage préférentiel

```{r functions_and_df_proposal}
get_phi <- function(x){
  sin(10 * x)^2 * exp(-5 * (x + 5)^2)
}
get_f <- function(x, log = FALSE){
  dnorm(x, log = log)
}
my_df_proposal <- 3
get_g <- function(x, df = my_df_proposal, log = FALSE){
  dt(x + 5, df = df, log = log) # Student distribution with finite variance
}
```

```{r function_tibble}
function_tibble <- tibble(x = seq(-8, 5, by = 0.001)) %>% 
  mutate(phi_x = get_phi(x), f_x = get_f(x), phi_f = phi_x * f_x,
         g_x = get_g(x), f_over_g = f_x / g_x) %>% 
  gather(-x, key = "Function", value = "Value", factor_key = TRUE) %>% 
  mutate(Function = factor(Function, labels = c("phi(x)", "f(x)",
                                                "phi(x)%*%f(x)",
                                                "g(x)", "f(x) %/% g(x)")))
```


## Exemple problématique:

On veut calculer
$$I = \mathbb{E}\left[\varphi(X) \right] = \int_{\mathbb{R}}\varphi(x)f(x)\text{d}x$$
où $X \sim \mathcal{N}(0, 1)$ (de densité $f(x)$) et 
$$\varphi(x) = \sin^2(x)\text{e}^{-5(x - 5)^2}$$

```{r plot_functions, fig.width =  7, fig.height = 3}
function_tibble %>% 
  filter(str_detect(Function, "g", negate = TRUE)) %>%
  ggplot(aes(x = x, y = Value)) +
  geom_line() +
  facet_wrap(.~Function, scales = "free", labeller = label_parsed)
```

## Estimateur Monte Carlo naturel

```{r get_mc_estimate}
get_mc_estimate <- function(n_samp){
  samples <- rnorm(n_samp)
  z_975 <- qnorm(0.975)
  tibble(index = 1:n_samp, x = samples, phi_x = get_phi(samples),
         w = 1) %>% 
    mutate(estimate = cumsum(phi_x) / index,
           variance = cumsum(phi_x^2) / index - estimate^2,
           borne_sup = estimate + z_975 * sqrt(variance / index),
           borne_inf = estimate - z_975 * sqrt(variance / index))
}
```

```{r my_mc_estimate}
set.seed(1234)
my_mc_estimate <- get_mc_estimate(5e4)
```

On tire $M = 50000$ points dans une loi $\mathcal{N}(0, 1)$, et on calcule la moyenne empirique.

## Résultats obtenus:

```{r resultats_estimation}
resultats <- read.table("resultats_estimation.csv",
                        sep = ",", header = T)
resultats %>% mutate(ecart_type = sqrt(variance_estimee / 5e4)) %>% 
  select(estimation, ecart_type) %>%
  filter(estimation < 0.1) %>% 
  gather(key = "Parametre", value = "Estimation", factor_key = TRUE) %>% 
  mutate(Parametre = factor(Parametre, labels = c("I", "sigma/M"))) %>% 
  ggplot(aes(y = Estimation)) +
  geom_boxplot() +
  scale_y_continuous(trans = "log10") +
  facet_wrap(.~Parametre, labeller = label_parsed) 
```

## Une trajectoire d'estimation

```{r plot_my_mc_estimate, fig.width =  7, fig.height = 3}
ggplot(my_mc_estimate) +
  aes(x = index, y = estimate) +
  geom_line() +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard")
```

L'estimation est très instable.

## Estimateur Monte Carlo naturel

Estimation de la variance et intervalle de confiance asymptotique:

```{r plot_my_mc_estimate_IC}
plot_mc_estimate <- ggplot(my_mc_estimate) +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup),
              fill = "lightblue") + 
  geom_line() +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard")
plot_mc_estimate
```

## Manque de chance?

```{r tree_monte_carlo_replicates}
set.seed(123)
three_mc_estimates <- rerun(3, get_mc_estimate(5e4)) %>% 
  bind_rows(.id = "Replicate")
ggplot(three_mc_estimates) +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup,
                  fill = Replicate), alpha = 0.5) + 
  geom_line(aes(group = Replicate, linetype = Replicate)) +
  theme(legend.position = "none") +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard") +
  coord_cartesian(ylim = c(-1, 1) * 5e-6)
```

## Origine du problème

```{r plot_monte_carlo_samples}
function_tibble %>% 
  filter(Function == "phi(x)%*%f(x)") %>% 
  ggplot(aes(x = x, y = Value)) +
  geom_line() +
  geom_point(data = my_mc_estimate,
             mapping = aes(y = 0), col = "blue",
             size = 0.5)
```

On échantillonne loin des régions importantes!

## Echantillonnage préférentiel

On cherche à estimer une intégrale du type:
$$I = \mathbb{E}_f[\varphi(X)] = \int_{\mathcal{D}_f} \varphi(x) f(x)\rmd x $$
où $\mathcal{D}_f \subset \R^d$, et $f$ est une densité de probabilité sur $\mathcal{D}_f$ (donc $f(x) = 0$ pour $x \not\in \mathcal{D}_f$) et $X$ une v.a. de loi $f$. \pause

Soit $g$ une densité de probabilité sur $\mathcal{D}_g \supseteq \mathcal{D}_f$ 
telle que $x\in \mathcal{D}_f \Rightarrow g(x) > 0$ et $Y$ une variable aléatoire de loi $g$, alors:
\begin{align*}
I &=  \int_{\mathcal{D}_f} \varphi(x) \frac{f(x)}{g(x)}g(x)\rmd x &\\ \pause
&= \int_{\mathcal{D}_g}  \varphi(x) \frac{f(x)}{g(x)}g(x)\rmd x & \text{ as } x \not\in \mathcal{D}_f \Rightarrow f(x) = 0\\ \pause
&= \mathbb{E}_g\left[\varphi(Y) \frac{f(Y)}{g(Y)}\right]&
\end{align*}

## Echantillonnage preferentiel

Comme estimateur de $I$, on peut ainsi proposer l'estimateur:
$$\hat{I}^{IS}_M = \frac{1}{M}\sum_{k = 1}^M \frac{f(Y_k)}{g(Y_k)} \varphi(Y_k)  = \frac{1}{M}\sum_{k = 1}^M W(Y_k)\varphi(Y_k)$$
où $Y_1,\dots,Y_M$ est un échantillon i.i.d. de variables aléatoires sur $\R^d$ de densité $g$.\pause

**Remarques:**

- Comme $Y_k \sim g$, $g(Y_k)$ est p.s. $\neq 0$\pause
- La variable aléatoire $W(Y_k) = \frac{f(Y_k)}{g(Y_k)}$ est appelée **poids d'importance** de $Y_k$. \pause
- Quand $f = g$, on a l'estimateur MC standard (et chaque poids vaut 1).

## Quel intérêt?

On peut choisir $g$ afin d'échantillonner dans les zones d'importance! 

**Biais:** 
\begin{align*}
\E_g[\hat{I}^{IS}_M]&=\frac{1}{M}\sum_{k=1}^M\int_{\mathcal{D}_g}  \frac{f(x)}{g(x)}\varphi(x) g(x) \rmd x\\
&=\int_{\mathcal{D}_f}  f(x)\varphi(x) \rmd x = I
\end{align*}
Donc, cet estimateur reste sans biais\pause

**Variance:**
$$\V_g[\hat{I}^{IS}_1] = \left[\E_g[\left(W(Y)\varphi(Y)\right)^2]  - I^2\right] = \int_{\mathcal{D}_g}\frac{\left(\varphi(y)f(y) - Ig(y)\right)^2}{g(y)}\rmd y$$

- La variance peut être très réduite si $g(y) \overset{\approx}{\propto} \varphi(y)f(y)$!
- Ceci peut guider le choix de $g$!

## Exemple

$g(x)$ est la densité d'une loi de Student $\mathcal{T}(`r my_df_proposal`)$.

```{r get_is_estimate}
get_is_estimate <- function(n_samp, df = my_df_proposal){
  samples <- rt(n_samp, df = df) - 5
  weights <- exp(get_f(samples, log = TRUE) - get_g(samples, df = df, log = TRUE))
  z_975 <- qnorm(0.975)
  tibble(index = 1:n_samp, x = samples, phi_x = get_phi(samples),
         w = weights) %>% 
    mutate(estimate = cumsum(phi_x * weights) / index,
           variance = cumsum((phi_x * weights)^2) / index - estimate^2,
           borne_sup = estimate + z_975 * sqrt(variance / index),
           borne_inf = estimate - z_975 * sqrt(variance / index))
}
```

```{r main_plot_is}
main_plot_is <- function_tibble %>% 
  filter(Function %in% c("phi(x)", "g(x)")) %>% 
  ggplot(aes(x = x, y = Value)) +
  scale_color_manual(labels = c(expression(phi(x), "g(x)")),
                     values = c("blue", "red")) +
  geom_line(aes(color = Function)) 
main_plot_is
```

## Exemple

```{r is_estimate_example}
set.seed(123)
M_example <- 30
is_estimate_example <- get_is_estimate(M_example) 
```

```{r plot_is_estimate_example}
main_plot_is +
  geom_point(data = is_estimate_example, aes(y = 0, size = w),
             color = "red") +
  guides(size = FALSE) +
  labs(title = paste0(M_example, " échantillons pondérés d'une Student(",
                      my_df_proposal, ")"))
```

## Estimation de $I$ (et IC asymptotique)

```{r plot_is_estimate}
set.seed(1234)
plot_is_estimate <- get_is_estimate(5e4) %>%  
  ggplot() +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup),
              fill = "coral2") +
  geom_line() + 
  labs(x = "M", y = expression(hat(IS)[M]^IS),
       title = "Estimation par échantillonnage préférentiel")
plot_is_estimate
```

## Comparaison sur cet exemple

```{r compare_is_mc}
set.seed(1)
mutate(get_mc_estimate(1e6), Type = "Standard") %>% 
  bind_rows(mutate(get_is_estimate(1e6), Type = "Ech. Pref")) %>% 
  slice(seq(1, 2e6, by = 100)) %>% 
  ggplot() +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup, fill = Type),
              alpha = 0.5) +
  geom_line(aes(color = Type)) + 
  labs(x = "M", y = expression(hat(I)),
       title = "Estimation de I")
```

## Echantillonnage préférentiel

### Avantages

- Très utile pour l'estimation de quantités petites (probabilités d'évènements rares). \pause
- Peut amener à une forte réduction de variance \pause
- Peut aussi être utiliser quand on ne sait pas simuler selon $f$! \pause

### Attention!

- Nécessite le choix de $g$! Pas toujours évident (notamment en grande dimension)!\pause
- Un mauvais $g$ peut amener à un estimateur de variance infinie! (voir TD).
- Un bon choix de $g$ est souvent "problème dépendant", (ne conviendra que pour $\E[\varphi(X)]$ pour un $\varphi$ spécifique).

# Echantillonnage préférentiel normalisé

## Problématique

Objectif, calculer:
$$I = \mathbb{E}[\varphi(X)] = \int_{\mathcal{D}_f} \varphi(x) f(x) d(x)$$

Supposons que $f$ ne soit connue qu'à une constante près:
$$f(x) = \frac{\overbrace{f^{(u)}(x)}^{\text{Connu}}}{\underbrace{\int_{\mathcal{D}_f} f^{(u)}(z)\rmd z}_\text{Inconnu}}.$$

- Pour une densité de proposition $g$, on ne peut plus calculer le poids d'importance!\pause

- Ce cas est en pratique très fréquent en inférence bayésienne!

## Echantillonnage préférentiel normalisé

Si on dispose d'une loi de proposition $g$ et de $Y_1,\dots, Y_M$ tirés indépendemment selon $g$,
alors l'estimateur:
$$\hat{I}^{IS,u}_M = \sum_{k = 1}^M  \frac{f^{(u)}(Y_k)/g(Y_k)}{\sum_{\ell = 1}^M f^{(u)}(Y_\ell) / g(Y_\ell)}\varphi(Y_k)$$
est un estimateur consistant (convergence en proba.) de $I$.\pause

- Estimateur biaisé pour $M$ petit;
- Peut amener à un estimateur de variance plus faible que l'échantillonnage préférentiel classique.

## Autres méthodes de réduction de variance

- Conditionnement 
- Variables de contrôles
- Quasi Monte Carlo

Voir références dans le poly.



